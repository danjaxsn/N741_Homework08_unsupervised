---
title: "N741 Homework 8 Due 04/24/2019"
author: "Melinda Higgins"
date: "April 17, 2019"
output: html_document
---

## Setup code

For this homework you will be working with the Abalone dataset from UCI, see [http://archive.ics.uci.edu/ml/datasets/abalone](http://archive.ics.uci.edu/ml/datasets/abalone). 

There were 3 dimensional measurements made of the abalones: length, diameter and height in mm (millimeters). Length was defined to be the longest length.

There were also 3 weight component measurements in grams: shucked weight, viscera weight and shell weight, which are all less than the total weight of the abalones. The total whole weight is also provided.

The "sexes" of the abalones in this dataset are:

* "I" for infants (immature)
* "M" for males (adult)
* "F" for females (adult)

We will use unsupervised clustering methods in this homework to see if any of the clusters and components reflect the abalones by sex.

The dataset also includes the number of rings in the shells which like trees can be used to determine the age of the abalones where age = rings + 1.5. We will not be using this measure in this homework.

The setup code below:

* reads in the data, 
* adds the variable names, 
* cleans up the dataset removing cases with typos or illogical measures that do not meet the criteria specified above; and 
* the code also does a small random sample of 5% of the cases, giving us about 200+ abalones to work with for this homework exercise.

```{r setup}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(readr)

# read-in abalone dataset
# abalone <- read_csv("abalone.data", col_names = FALSE)
abalone <- read_csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"))

# assign variable names
names(abalone) <- c("sex","length","diameter","height",
                    "wholeWt","shuckedWt","visceraWt","shellWt",
                    "rings")

# cleanup data
# component weights should be less than whole weight
# length is defined to be the longest dimensional measurement
# and all dimensions should be > 0 - there are some 0 heights
abaloneClean <- abalone %>%
  filter(shuckedWt < wholeWt) %>%
  filter(visceraWt < wholeWt) %>%
  filter(diameter < length) %>%
  filter(height < length) %>%
  filter(height > 0)

# take a 5% random sample of 4169 cases
abaloneSample <- abaloneClean %>%
  sample_frac(size = 0.05, replace = FALSE)

# keep 3 dimensions and 3 component weights
# remove sex and number of rings
# number of rings + 1.5 = age of abalone
abaloneKeep <- abaloneSample %>%
  select(length, diameter, height, 
         shuckedWt, visceraWt, shellWt)
```

You will use the `abaloneKeep` dataset for this homework exercise, which has about 200+ cases and only has 6 variables: 3 dimensional measures and 3 component weights. 

Use the code from the Unsupervised Learning lecture to guide you in this homework - see  [https://htmlpreview.github.io/?https://github.com/vhertzb/more-supervised-learning/blob/master/More_Supervised_Learning.html](https://htmlpreview.github.io/?https://github.com/vhertzb/more-supervised-learning/blob/master/More_Supervised_Learning.html) - look about half-way down the webpage.

## Cluster Analysis

```{r}
# Scale the data before clustering
sd.data <- scale(abaloneKeep)
```

PROBLEM 1: Using the scaled data `sd.data` calculate the Euclidean distance between each pair of points

```{r}
# insert code here
```

PROBLEM 2: Plot the tree, default linkage = 'complete'. Set `labels = abaloneSample$sex` to get the sex labels for the plot.

```{r}
# insert code here
```

PROBLEM 3: Plot the tree, using linkage = 'average'. Set `labels = abaloneSample$sex` to get the sex labels for the plot.

```{r}
# insert code here
```

PROBLEM 4: Plot the tree, using default linkage = 'single'. Set `labels = abaloneSample$sex` to get the sex labels for the plot.

```{r}
# insert code here
```

PROBLEM 5: Let's use complete linkage and cut into 3 clusters. When running the `table()` line of code use `abaloneSample$sex` to compare the original sexes to the clusters `hc.clusters`.

```{r}
# insert code here
```

PROBLEM 6: Given the results above:

* which cluster(s) did the best job identifying the infants?
* which cluster(s) found the adults (M and F)?

## K-Means clustering

PROBLEM 7: Run a K-means clustering with K=3 (same as we did above for the hierarchical clustering). Compare the clusters from K-means clustering and the hierarchical clustering `hc.clusters` (using the complete linkage method). How do these clusters compare - similar or not?

```{r}
# insert code here
```

## PCA


## MDS





